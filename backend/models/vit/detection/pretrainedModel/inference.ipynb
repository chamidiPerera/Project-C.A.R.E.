{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'torchvision.models.vision_transformer.VisionTransformer'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m      9\u001b[0m vit_model \u001b[38;5;241m=\u001b[39m ViT(\n\u001b[1;32m     10\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     11\u001b[0m     patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     emb_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_vit_model_skin_or_eye_detection.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mvit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m vit_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Define transformations\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:2104\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \n\u001b[1;32m   2071\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2106\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2107\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'torchvision.models.vision_transformer.VisionTransformer'>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from vit_pytorch import ViT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ViT model\n",
    "vit_model = ViT(\n",
    "    image_size=256,\n",
    "    patch_size=32,\n",
    "    num_classes=1000,\n",
    "    dim=1024,\n",
    "    depth=6,\n",
    "    heads=16,\n",
    "    mlp_dim=2048,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ")\n",
    "checkpoint = torch.load(\"pretrained_vit_model_skin_or_eye_detection.pth\")\n",
    "vit_model.load_state_dict(checkpoint)\n",
    "vit_model.eval()\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Function to predict class and confidence level\n",
    "def predict(image_path, model):\n",
    "    img = Image.open(image_path)\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "    probs = torch.nn.functional.softmax(output, dim=-1)[0] * 100\n",
    "    confidence, predicted_class = torch.max(probs, dim=-1)\n",
    "    return predicted_class.item(), confidence.item()\n",
    "\n",
    "# Function to display image, class, and confidence\n",
    "def display_prediction(image_path, model):\n",
    "    predicted_class, confidence = predict(image_path, model)\n",
    "    classes = np.loadtxt(\"imagenet_classes.txt\", str, delimiter='\\t')\n",
    "    img = Image.open(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Prediction: {classes[predicted_class]} - Confidence: {confidence:.2f}%')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the image path\n",
    "image_path = \"skin.jpg\"\n",
    "display_prediction(image_path, vit_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
